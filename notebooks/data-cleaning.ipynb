{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the DESeq2 data from the PACE cohort\n",
    "### Matthew Muller\n",
    "11/24/2022\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "##########\n",
    "# Set Working directory\n",
    "\n",
    "# %cd ..\n",
    "\n",
    "##########\n",
    "# Import Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Training Data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pace_features = pd.read_csv('data/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pace_features \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# median of ratios counts of genes # this is press_1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# pace_features = pd.read_csv('/Users/muller/Ruggles Lab Dropbox/Matthew Muller/projects/platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press_2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Metadata\u001b[39;00m\n\u001b[1;32m      8\u001b[0m pace_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/hypercohort_metatable.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Metadata on patients\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/main/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/main/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/main/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/main/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/main/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv'"
     ]
    }
   ],
   "source": [
    "## Training Data\n",
    "# pace_features = pd.read_csv('data/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press\n",
    "pace_features = pd.read_csv('../platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press_1\n",
    "# pace_features = pd.read_csv('/Users/muller/Ruggles Lab Dropbox/Matthew Muller/projects/platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press_2\n",
    "\n",
    "\n",
    "# Metadata\n",
    "pace_metadata = pd.read_csv('data/hypercohort_metatable.csv') # Metadata on patients\n",
    "\n",
    "# Subset of 3 key genes that Jeffrey found separately\n",
    "key_3_genes = pd.read_csv('data/clean/key_3_genes.csv', header=None) # All Up and Down regulated genes\n",
    "top_20_genes = pd.read_csv('data/clean/model_top20_genes.csv', header=0) # Genes selected by the forest model\n",
    "\n",
    "\n",
    "# Low dose LTA values for regression making\n",
    "pace_lta = pace_features[['epi_04um_300s_n']].rename({'epi_04um_300s_n':'Low-dose LTA'}, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duke Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation Data\n",
    "duke_features = pd.read_csv('data/duke_validation_run3/normcounttab_genesymbols.csv', index_col=0)\n",
    "\n",
    "# subset the duke features\n",
    "duke_features = duke_features.T[ duke_features.columns.isin(pace_features.columns) ].T \n",
    "\n",
    "# subset the pace features now\n",
    "tmp_columns = set(pace_features.columns) - set(duke_features.columns)\n",
    "for gene in tmp_columns:\n",
    "    duke_features[gene]=0\n",
    "pace_features_clean = pace_features.drop(['epi_04um_300s_n', 'hypercohort'], axis=1)\n",
    "\n",
    "# pace_features_clean = pace_features.T[ pace_features.columns.isin(duke_features.columns) ].T \n",
    "# pace_features_clean = pace_features.T[ pace_features.columns.isin(pace_all_genes['0']) ].T # Needed if using the forest model genes\n",
    "\n",
    "pace_labels_clean = label_binarize(pace_features[['hypercohort']], # Clean up labels\n",
    "                                   classes=[\"nothyper\", \"hyper\"])\n",
    "\n",
    "# subset the duke features again\n",
    "duke_features = duke_features.T[ duke_features.columns.isin(pace_features_clean.columns) ].T \n",
    "\n",
    "# The duke labels are split into group 1 and group 2\n",
    "# Both of these groups are not given medication (1)\n",
    "#  or not on medication at the given time (2) EDIT: group 2 is garbage\n",
    "duke_labels_group_1 = pd.read_csv('data/duke_validation_run3/cohort_descriptions/group1plottable1.csv', \n",
    "                                  index_col=0)[['compouttable[, 2]']] # group 1\n",
    "duke_labels_group_2 = pd.read_csv('data/duke_validation_run3/cohort_descriptions/group2plottable1.csv', index_col=0)[['compouttable[, 2]']] # group 2 (this group had NAs)\n",
    "\n",
    "# divide up the duke_features dataframe to make group 1 and group 2\n",
    "# subset the groups\n",
    "duke_features_group_1 = duke_features[ duke_features.index.isin(duke_labels_group_1.index) ] \n",
    "duke_features_group_2 = duke_features[ duke_features.index.isin(duke_labels_group_2.index) ]\n",
    "# sort the groups\n",
    "duke_features_group_1 = duke_features_group_1.reindex(sorted(duke_features_group_1.columns), axis=1) \n",
    "duke_features_group_2 = duke_features_group_2.reindex(sorted(duke_features_group_2.columns), axis=1)\n",
    "\n",
    "## Regression Low-dose LTA values\n",
    "duke_metadata = pd.read_csv('data/duke_validation_run3/dukemetatable_sel.csv')\n",
    "duke_group_1_metadata = duke_metadata.loc[ duke_metadata['cohort']== 'group1' ]\n",
    "duke_group_1_lta = duke_group_1_metadata[['characteristic__epi_max_05']].rename({'characteristic__epi_max_05':'Low-dose LTA'})\n",
    "\n",
    "\n",
    "\n",
    "##### Determine a duke cohort consistent between group 1 and 2 ######\n",
    "# ie. a longitudinal duke cohort\n",
    "\n",
    "# duke_intersection\n",
    "duke_metadata_g1 = duke_metadata.loc[(duke_metadata['cohort'] == 'group1')].set_index('characteristic__subject_id').sort_index()\n",
    "duke_metadata_g2 = duke_metadata.loc[(duke_metadata['cohort'] == 'group2')].set_index('characteristic__subject_id').sort_index()\n",
    "group_intersect = list(set(duke_metadata_g1.index).intersection(set(duke_metadata_g2.index)))\n",
    "duke_metadata_g1 = duke_metadata_g1.loc[group_intersect]\n",
    "duke_metadata_g2 = duke_metadata_g2.loc[group_intersect]\n",
    "\n",
    "\n",
    "duke_hyper = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "duke_norm = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "\n",
    "subjects = pd.concat([duke_hyper, duke_norm])['Unnamed: 0']\n",
    "\n",
    "duke_features_group_1 = duke_features_group_1.loc[subjects]\n",
    "duke_labels_group_1 = duke_labels_group_1.loc[subjects]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "duke_hyper = duke_metadata_g2.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "duke_norm = duke_metadata_g2.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "subjects2 = pd.concat([duke_hyper, duke_norm])['Unnamed: 0']\n",
    "\n",
    "duke_features_group_2 = duke_features_group_2.loc[subjects2]\n",
    "duke_labels_group_2 = duke_labels_group_2.loc[subjects2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull the Duke metadata for Tessa for a figure update.\n",
    "# Probably not needed otherwise.\n",
    "# duke_hyper = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "# duke_norm = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "\n",
    "# subjects = pd.concat([duke_hyper, duke_norm])\n",
    "# subjects['label'] = list(duke_labels_group_1['compouttable[, 2]'].map({1:'hyper', 0:'normal'}))\n",
    "\n",
    "# subjects.to_csv('/Users/muller/Desktop/duke_samples.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duke and Pace Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined Dataset to test how well we can perform here.\n",
    "# Make a merged duke labels set\n",
    "\n",
    "# Removed Duke group 2 due to messy data\n",
    "# duke_labels = duke_labels_group_1.T.merge(duke_labels_group_2.T, left_index=True, right_index=True).T\n",
    "# features = duke_features.T.merge(pace_features_clean.T, right_index=True, left_index=True).T\n",
    "# labels = pd.concat([ duke_labels['compouttable[, 2]'], pace_features['hypercohort'].map({'nothyper':0, 'hyper':1}) ])\n",
    "\n",
    "features = duke_features_group_1.T.merge(pace_features_clean.T, right_index=True, left_index=True).T\n",
    "labels = pd.concat([ duke_labels_group_1['compouttable[, 2]'], pace_features['hypercohort'].map({'nothyper':0, 'hyper':1}) ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at subsets of PRESS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the key_3_genes features\n",
    "# right now I am missing one gene?\n",
    "key_pace_features = pace_features.T[ pace_features.columns.isin(key_3_genes.values.flatten()) ].T\n",
    "key_duke_features = duke_features_group_1.T[ duke_features_group_1.columns.isin(key_pace_features.columns) ].T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean counts in a matrix for input into SKLearn\n",
    "# Pace Data (Training)\n",
    "# pace_features_clean.to_csv('data/clean/pace/features.csv', index=None)\n",
    "pace_features_clean.to_csv('data/clean/pace/features.csv')\n",
    "\n",
    "pd.DataFrame(pace_labels_clean).to_csv('data/clean/pace/labels.csv', index=None)\n",
    "\n",
    "# Duke Data (Validation)\n",
    "duke_features_group_1.to_csv('data/clean/duke/features_group1.csv', index=None)\n",
    "duke_features_group_2.to_csv('data/clean/duke/features_group2.csv', index=None)\n",
    "\n",
    "duke_labels_group_1.to_csv('data/clean/duke/labels_group1.csv', index=None)\n",
    "duke_labels_group_2.to_csv('data/clean/duke/labels_group2.csv', index=None)\n",
    "\n",
    "# duke_features.to_csv('data/clean/duke/features.csv', index=None)\n",
    "# duke_labels.to_csv('data/clean/duke/labels.csv', index=None)\n",
    "\n",
    "\n",
    "# Key 3 Gene Features\n",
    "key_pace_features.to_csv('data/clean/pace/key_features.csv', index=None)\n",
    "key_duke_features.to_csv('data/clean/duke/key_features.csv', index=None)\n",
    "\n",
    "\n",
    "## Send to csv\n",
    "features.to_csv('data/clean/combined_features.csv', index=None)\n",
    "labels.to_csv('data/clean/combined_labels.csv', index=None)\n",
    "\n",
    "\n",
    "## Regression truth values\n",
    "pace_lta.to_csv('data/clean/pace/lta_values.csv', index=None)\n",
    "duke_group_1_lta.to_csv('data/clean/duke/lta_values.csv', index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Duke 1 Features: (35, 451) \n",
      " Duke 1 Labels  : (35, 1) \n",
      "\n",
      " Duke 2 Features: (35, 451) \n",
      " Duke 2 Labels  : (35, 1) \n",
      "\n",
      " Pace Features: (84, 451) \n",
      " Pace Labels  : (84, 1) \n",
      "\n",
      " Pace Key Features  : (84, 2) \n",
      " Duke Key Features : (35, 2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compouttable[, 2]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DVS132</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S47R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S62</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.85</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.114</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.120R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.147R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.159</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.182</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS192</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.206</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS283</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS292</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X352</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.32R</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S37R</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S55</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S70</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S81</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.90</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.100</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.116</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.109</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.165</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.187</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.196</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.201</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.221</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.228</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS268</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS288</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X322</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X332</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X342</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X347</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          compouttable[, 2]\n",
       "DVS132                  1.0\n",
       "DV.S47R                 1.0\n",
       "DV.S62                  1.0\n",
       "DVS.85                  1.0\n",
       "DVS.114                 1.0\n",
       "DVS.120R                1.0\n",
       "DVS.147R                1.0\n",
       "DVS.159                 1.0\n",
       "DVS.182                 1.0\n",
       "DVS192                  1.0\n",
       "DVS.206                 1.0\n",
       "DVS283                  1.0\n",
       "DVS292                  1.0\n",
       "X352                    1.0\n",
       "DVS.32R                 0.0\n",
       "DV.S37R                 0.0\n",
       "DV.S55                  0.0\n",
       "DV.S70                  0.0\n",
       "DV.S81                  0.0\n",
       "DVS.90                  0.0\n",
       "DVS.100                 0.0\n",
       "DVS.116                 0.0\n",
       "DVS.109                 0.0\n",
       "DVS.165                 0.0\n",
       "DVS.187                 0.0\n",
       "DVS.196                 0.0\n",
       "DVS.201                 0.0\n",
       "DVS.221                 0.0\n",
       "DVS.228                 0.0\n",
       "DVS268                  0.0\n",
       "DVS288                  0.0\n",
       "X322                    0.0\n",
       "X332                    0.0\n",
       "X342                    0.0\n",
       "X347                    0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Take a look at things so far:\n",
    "print(f' Duke 1 Features: {duke_features_group_1.shape} \\n',\n",
    "      f'Duke 1 Labels  : {duke_labels_group_1.shape} \\n')\n",
    "\n",
    "print(f' Duke 2 Features: {duke_features_group_2.shape} \\n',\n",
    "      f'Duke 2 Labels  : {duke_labels_group_2.shape} \\n')\n",
    "\n",
    "print(f' Pace Features: {pace_features_clean.shape} \\n',\n",
    "      f'Pace Labels  : {pace_labels_clean.shape} \\n')\n",
    "\n",
    "print(f' Pace Key Features  : {key_pace_features.shape} \\n',\n",
    "      f'Duke Key Features : {key_duke_features.shape} \\n')\n",
    "\n",
    "# Save the genes just in case\n",
    "genes = duke_features.columns.to_numpy()\n",
    "np.savetxt(\"data/clean/press_genes.csv\",\n",
    "           genes, delimiter=\", \", fmt ='% s')\n",
    "\n",
    "# Save the group of consistent hyper dukers\n",
    "# subjects.to_csv('data/clean/duke_longitudinal_group.csv')\n",
    "\n",
    "pd.concat([duke_hyper, duke_norm])[['Unnamed: 0', 'characteristic__epi_max_05']].reset_index().to_csv('output/data-wrangling__2023-02-15/duke_group1_group2_longitudinal.csv')\n",
    "\n",
    "duke_labels_group_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d66913bb719389121bab79020f7a52c6e05d87274bae621f301af382c19c893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
