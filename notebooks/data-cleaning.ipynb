{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the DESeq2 data from the PACE cohort\n",
    "### Matthew Muller\n",
    "11/24/2022\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muller/Library/CloudStorage/GoogleDrive-mm12865@nyu.edu/My Drive/RugglesLab/projects/platelet-activity\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "##########\n",
    "# Set Working directory\n",
    "\n",
    "%cd ..\n",
    "\n",
    "##########\n",
    "# Import Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Data\n",
    "# pace_features = pd.read_csv('data/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press\n",
    "pace_features = pd.read_csv('/Users/muller/Library/CloudStorage/GoogleDrive-mm12865@nyu.edu/My Drive/RugglesLab/projects/platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press_1\n",
    "# pace_features = pd.read_csv('/Users/muller/Ruggles Lab Dropbox/Matthew Muller/projects/platelet-pace/output/hyper_geneset_creation/run15_hyper60_hypo40_AGRCONTROL/hyper_feature_outtable.csv', index_col=0) # median of ratios counts of genes # this is press_2\n",
    "\n",
    "\n",
    "# Metadata\n",
    "pace_metadata = pd.read_csv('data/hypercohort_metatable.csv') # Metadata on patients\n",
    "\n",
    "# Subset of 3 key genes that Jeffrey found separately\n",
    "key_3_genes = pd.read_csv('data/clean/key_3_genes.csv', header=None) # All Up and Down regulated genes\n",
    "top_20_genes = pd.read_csv('data/clean/model_top20_genes.csv', header=0) # Genes selected by the forest model\n",
    "\n",
    "\n",
    "# Low dose LTA values for regression making\n",
    "pace_lta = pace_features[['epi_04um_300s_n']].rename({'epi_04um_300s_n':'Low-dose LTA'}, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duke Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation Data\n",
    "duke_features = pd.read_csv('data/duke_validation_run3/normcounttab_genesymbols.csv', index_col=0)\n",
    "\n",
    "# subset the duke features\n",
    "duke_features = duke_features.T[ duke_features.columns.isin(pace_features.columns) ].T \n",
    "\n",
    "# subset the pace features now\n",
    "tmp_columns = set(pace_features.columns) - set(duke_features.columns)\n",
    "for gene in tmp_columns:\n",
    "    duke_features[gene]=0\n",
    "pace_features_clean = pace_features.drop(['epi_04um_300s_n', 'hypercohort'], axis=1)\n",
    "\n",
    "# pace_features_clean = pace_features.T[ pace_features.columns.isin(duke_features.columns) ].T \n",
    "# pace_features_clean = pace_features.T[ pace_features.columns.isin(pace_all_genes['0']) ].T # Needed if using the forest model genes\n",
    "\n",
    "pace_labels_clean = label_binarize(pace_features[['hypercohort']], # Clean up labels\n",
    "                                   classes=[\"nothyper\", \"hyper\"])\n",
    "\n",
    "# subset the duke features again\n",
    "duke_features = duke_features.T[ duke_features.columns.isin(pace_features_clean.columns) ].T \n",
    "\n",
    "# The duke labels are split into group 1 and group 2\n",
    "# Both of these groups are not given medication (1)\n",
    "#  or not on medication at the given time (2) EDIT: group 2 is garbage\n",
    "duke_labels_group_1 = pd.read_csv('data/duke_validation_run3/cohort_descriptions/group1plottable1.csv', \n",
    "                                  index_col=0)[['compouttable[, 2]']] # group 1\n",
    "duke_labels_group_2 = pd.read_csv('data/duke_validation_run3/cohort_descriptions/group2plottable1.csv', index_col=0)[['compouttable[, 2]']] # group 2 (this group had NAs)\n",
    "\n",
    "# divide up the duke_features dataframe to make group 1 and group 2\n",
    "# subset the groups\n",
    "duke_features_group_1 = duke_features[ duke_features.index.isin(duke_labels_group_1.index) ] \n",
    "duke_features_group_2 = duke_features[ duke_features.index.isin(duke_labels_group_2.index) ]\n",
    "# sort the groups\n",
    "duke_features_group_1 = duke_features_group_1.reindex(sorted(duke_features_group_1.columns), axis=1) \n",
    "duke_features_group_2 = duke_features_group_2.reindex(sorted(duke_features_group_2.columns), axis=1)\n",
    "\n",
    "## Regression Low-dose LTA values\n",
    "duke_metadata = pd.read_csv('data/duke_validation_run3/dukemetatable_sel.csv')\n",
    "duke_group_1_metadata = duke_metadata.loc[ duke_metadata['cohort']== 'group1' ]\n",
    "duke_group_1_lta = duke_group_1_metadata[['characteristic__epi_max_05']].rename({'characteristic__epi_max_05':'Low-dose LTA'})\n",
    "\n",
    "\n",
    "\n",
    "##### Determine a duke cohort consistent between group 1 and 2 ######\n",
    "# ie. a longitudinal duke cohort\n",
    "\n",
    "# duke_intersection\n",
    "duke_metadata_g1 = duke_metadata.loc[(duke_metadata['cohort'] == 'group1')].set_index('characteristic__subject_id').sort_index()\n",
    "duke_metadata_g2 = duke_metadata.loc[(duke_metadata['cohort'] == 'group2')].set_index('characteristic__subject_id').sort_index()\n",
    "group_intersect = list(set(duke_metadata_g1.index).intersection(set(duke_metadata_g2.index)))\n",
    "duke_metadata_g1 = duke_metadata_g1.loc[group_intersect]\n",
    "duke_metadata_g2 = duke_metadata_g2.loc[group_intersect]\n",
    "\n",
    "\n",
    "duke_hyper = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "duke_norm = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "\n",
    "subjects = pd.concat([duke_hyper, duke_norm])['Unnamed: 0']\n",
    "\n",
    "duke_features_group_1 = duke_features_group_1.loc[subjects]\n",
    "duke_labels_group_1 = duke_labels_group_1.loc[subjects]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "duke_hyper = duke_metadata_g2.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "duke_norm = duke_metadata_g2.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "subjects2 = pd.concat([duke_hyper, duke_norm])['Unnamed: 0']\n",
    "\n",
    "duke_features_group_2 = duke_features_group_2.loc[subjects2]\n",
    "duke_labels_group_2 = duke_labels_group_2.loc[subjects2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull the Duke metadata for Tessa for a figure update.\n",
    "# Probably not needed otherwise.\n",
    "# duke_hyper = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] > 60) & (duke_metadata_g2['characteristic__epi_max_05'] > 60)]\n",
    "# duke_norm = duke_metadata_g1.loc[(duke_metadata_g1['characteristic__epi_max_05'] < 40) & (duke_metadata_g2['characteristic__epi_max_05'] < 40)]\n",
    "\n",
    "# subjects = pd.concat([duke_hyper, duke_norm])\n",
    "# subjects['label'] = list(duke_labels_group_1['compouttable[, 2]'].map({1:'hyper', 0:'normal'}))\n",
    "\n",
    "# subjects.to_csv('/Users/muller/Desktop/duke_samples.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duke and Pace Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined Dataset to test how well we can perform here.\n",
    "# Make a merged duke labels set\n",
    "\n",
    "# Removed Duke group 2 due to messy data\n",
    "# duke_labels = duke_labels_group_1.T.merge(duke_labels_group_2.T, left_index=True, right_index=True).T\n",
    "# features = duke_features.T.merge(pace_features_clean.T, right_index=True, left_index=True).T\n",
    "# labels = pd.concat([ duke_labels['compouttable[, 2]'], pace_features['hypercohort'].map({'nothyper':0, 'hyper':1}) ])\n",
    "\n",
    "features = duke_features_group_1.T.merge(pace_features_clean.T, right_index=True, left_index=True).T\n",
    "labels = pd.concat([ duke_labels_group_1['compouttable[, 2]'], pace_features['hypercohort'].map({'nothyper':0, 'hyper':1}) ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at subsets of PRESS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the key_3_genes features\n",
    "# right now I am missing one gene?\n",
    "key_pace_features = pace_features.T[ pace_features.columns.isin(key_3_genes.values.flatten()) ].T\n",
    "key_duke_features = duke_features_group_1.T[ duke_features_group_1.columns.isin(key_pace_features.columns) ].T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean counts in a matrix for input into SKLearn\n",
    "# Pace Data (Training)\n",
    "# pace_features_clean.to_csv('data/clean/pace/features.csv', index=None)\n",
    "pace_features_clean.to_csv('data/clean/pace/features.csv')\n",
    "\n",
    "pd.DataFrame(pace_labels_clean).to_csv('data/clean/pace/labels.csv', index=None)\n",
    "\n",
    "# Duke Data (Validation)\n",
    "duke_features_group_1.to_csv('data/clean/duke/features_group1.csv', index=None)\n",
    "duke_features_group_2.to_csv('data/clean/duke/features_group2.csv', index=None)\n",
    "\n",
    "duke_labels_group_1.to_csv('data/clean/duke/labels_group1.csv', index=None)\n",
    "duke_labels_group_2.to_csv('data/clean/duke/labels_group2.csv', index=None)\n",
    "\n",
    "# duke_features.to_csv('data/clean/duke/features.csv', index=None)\n",
    "# duke_labels.to_csv('data/clean/duke/labels.csv', index=None)\n",
    "\n",
    "\n",
    "# Key 3 Gene Features\n",
    "key_pace_features.to_csv('data/clean/pace/key_features.csv', index=None)\n",
    "key_duke_features.to_csv('data/clean/duke/key_features.csv', index=None)\n",
    "\n",
    "\n",
    "## Send to csv\n",
    "features.to_csv('data/clean/combined_features.csv', index=None)\n",
    "labels.to_csv('data/clean/combined_labels.csv', index=None)\n",
    "\n",
    "\n",
    "## Regression truth values\n",
    "pace_lta.to_csv('data/clean/pace/lta_values.csv', index=None)\n",
    "duke_group_1_lta.to_csv('data/clean/duke/lta_values.csv', index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Duke 1 Features: (35, 451) \n",
      " Duke 1 Labels  : (35, 1) \n",
      "\n",
      " Duke 2 Features: (35, 451) \n",
      " Duke 2 Labels  : (35, 1) \n",
      "\n",
      " Pace Features: (84, 451) \n",
      " Pace Labels  : (84, 1) \n",
      "\n",
      " Pace Key Features  : (84, 2) \n",
      " Duke Key Features : (35, 2) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compouttable[, 2]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DVS132</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S47R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S62</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.85</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.114</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.120R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.147R</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.159</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.182</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS192</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.206</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS283</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS292</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X352</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.32R</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S37R</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S55</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S70</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DV.S81</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.90</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.100</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.116</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.109</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.165</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.187</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.196</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.201</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.221</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS.228</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS268</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVS288</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X322</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X332</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X342</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X347</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          compouttable[, 2]\n",
       "DVS132                  1.0\n",
       "DV.S47R                 1.0\n",
       "DV.S62                  1.0\n",
       "DVS.85                  1.0\n",
       "DVS.114                 1.0\n",
       "DVS.120R                1.0\n",
       "DVS.147R                1.0\n",
       "DVS.159                 1.0\n",
       "DVS.182                 1.0\n",
       "DVS192                  1.0\n",
       "DVS.206                 1.0\n",
       "DVS283                  1.0\n",
       "DVS292                  1.0\n",
       "X352                    1.0\n",
       "DVS.32R                 0.0\n",
       "DV.S37R                 0.0\n",
       "DV.S55                  0.0\n",
       "DV.S70                  0.0\n",
       "DV.S81                  0.0\n",
       "DVS.90                  0.0\n",
       "DVS.100                 0.0\n",
       "DVS.116                 0.0\n",
       "DVS.109                 0.0\n",
       "DVS.165                 0.0\n",
       "DVS.187                 0.0\n",
       "DVS.196                 0.0\n",
       "DVS.201                 0.0\n",
       "DVS.221                 0.0\n",
       "DVS.228                 0.0\n",
       "DVS268                  0.0\n",
       "DVS288                  0.0\n",
       "X322                    0.0\n",
       "X332                    0.0\n",
       "X342                    0.0\n",
       "X347                    0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Take a look at things so far:\n",
    "print(f' Duke 1 Features: {duke_features_group_1.shape} \\n',\n",
    "      f'Duke 1 Labels  : {duke_labels_group_1.shape} \\n')\n",
    "\n",
    "print(f' Duke 2 Features: {duke_features_group_2.shape} \\n',\n",
    "      f'Duke 2 Labels  : {duke_labels_group_2.shape} \\n')\n",
    "\n",
    "print(f' Pace Features: {pace_features_clean.shape} \\n',\n",
    "      f'Pace Labels  : {pace_labels_clean.shape} \\n')\n",
    "\n",
    "print(f' Pace Key Features  : {key_pace_features.shape} \\n',\n",
    "      f'Duke Key Features : {key_duke_features.shape} \\n')\n",
    "\n",
    "# Save the genes just in case\n",
    "genes = duke_features.columns.to_numpy()\n",
    "np.savetxt(\"data/clean/press_genes.csv\",\n",
    "           genes, delimiter=\", \", fmt ='% s')\n",
    "\n",
    "# Save the group of consistent hyper dukers\n",
    "# subjects.to_csv('data/clean/duke_longitudinal_group.csv')\n",
    "\n",
    "pd.concat([duke_hyper, duke_norm])[['Unnamed: 0', 'characteristic__epi_max_05']].reset_index().to_csv('output/data-wrangling__2023-02-15/duke_group1_group2_longitudinal.csv')\n",
    "\n",
    "duke_labels_group_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d66913bb719389121bab79020f7a52c6e05d87274bae621f301af382c19c893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
