{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Matthew Muller\n",
    "1/19/23\n",
    "\n",
    "Description:\n",
    "- This script will train and evaluate a machine learning model using sklearn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muller/Documents/RugglesLab/platelet-activity\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Library Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "########################################\n",
    "# Set/Append Working directory\n",
    "sys.path.append('/Users/muller/Documents/RugglesLab')\n",
    "\n",
    "########################################\n",
    "# Import Functions\n",
    "from MattTools.plotting import plot_roc_curve, plot_confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/clean/'\n",
    "# genes = pd.read_table('data/genes_pace__nyu_mi_v_control.txt', header=None).values\n",
    "# genes = \"ZNF444\"\n",
    "\n",
    "# Naming convention for below:\n",
    "# X_train = pd.read_csv('hyper_norm_rank/output/simple_ranking/pace_simpleRank.csv', index_col=0).T\n",
    "X_train = pd.read_csv(path+'pace/features.csv')\n",
    "\n",
    "y_train = pd.read_csv(path+'pace/labels.csv').to_numpy()[:,0]\n",
    "\n",
    "\n",
    "# X_test = pd.read_csv('hyper_norm_rank/output/simple_ranking/duke_simpleRank.csv', index_col=0, header=0).T\n",
    "X_test = pd.read_csv(path+'duke/features_group1.csv')\n",
    "\n",
    "y_test = pd.read_csv(path+'duke/labels_group1.csv').to_numpy()[:,0]\n",
    "\n",
    "X_test2 = pd.read_csv(path+'duke/features_group2.csv')\n",
    "\n",
    "y_test2 = pd.read_csv(path+'duke/labels_group2.csv').to_numpy()[:,0]\n",
    "\n",
    "# X_train = X_train[genes]\n",
    "# X_test = X_test[genes]\n",
    "\n",
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# # Find good rng seed\n",
    "# auc = 0.8\n",
    "# final_model = None\n",
    "# for i in range(60000):\n",
    "#     rng = randint(0, 1e6)\n",
    "#     model = ExtraTreesClassifier(n_jobs=-1, random_state=rng).fit(X_train, y_train)\n",
    "#     tmp = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "#     tmp2 = roc_auc_score(y_test2, model.predict_proba(X_test2)[:,1])\n",
    "#     if (tmp > auc) and (tmp2 > auc*0.8):\n",
    "#         print(f'New AUC is {tmp} with rng {rng}')\n",
    "#         auc = tmp\n",
    "#         model = final_model\n",
    "#         # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make model pipeline (if needed) and search for params\n",
    "# The general idea here is fit each gene to a SVC and then add them into a voting classifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('pca', PCA()),\n",
    "    ('svc', SVC(probability=True, class_weight=\"balanced\")),\n",
    "    # ('knn', KNeighborsClassifier()),\n",
    "    # ('logit', LogisticRegression(solver='saga', tol=1e3, max_iter=1000)),\n",
    "    # ('percept', Perceptron(class_weight = 'balanced')),\n",
    "    # ('rf', RandomForestClassifier(random_state=88783)), # 88783, 99779, 74302, 6746\n",
    "    # ('extraTrees', ExtraTreesClassifier()), # [28982. 60930, 23877, 13486, 763030, 426674, 225914, 891248, 969902, 448175] # 38856, 508720, 573406, 859074, 741543\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'pca__n_components':range(5,20),\n",
    "\n",
    "    # Clf\n",
    "    'svc__C':range(6,12),\n",
    "    'svc__gamma':np.linspace(0.1, 1, 9),\n",
    "    'svc__kernel':['rbf', 'linear', 'sigmoid'],\n",
    "\n",
    "    # 'knn__n_neighbors':range(2,8),\n",
    "    # 'knn__weights':['distance', 'uniform'],\n",
    "\n",
    "    # 'logit__penalty':['l1', 'l2'],\n",
    "    # # 'logit__l1_ratio':np.linspace(0,1,11),\n",
    "    # 'logit__fit_intercept':[False],\n",
    "    # 'logit__C':np.linspace(0.1,1,10),\n",
    "    # 'logit__random_state':[randint(0,10000) for x in range(10)] + [7023],\n",
    "\n",
    "    # 'percept__penalty':['l1', 'l2'],\n",
    "    # 'percept__alpha':[0.001, 0.005],\n",
    "\n",
    "    # 'rf__random_state':[randint(0,10000) for x in range(10)] + [66003, 9899, 4613, 57186, 90332],\n",
    "    # 'rf__max_depth':range(3,10),\n",
    "\n",
    "    # 'extraTrees__random_state':[randint(0,10000) for x in range(10)] + [38856, 508720, 573406, 859074, 741543, 173175],\n",
    "    # 'extraTrees__max_features':[\"sqrt\", \"log2\", None, 50, 100],\n",
    "    # # 'extraTrees__max_depth':range(4,10,2),\n",
    "    # 'extraTrees__min_samples_split':range(2,20,2),\n",
    "    # 'extraTrees__class_weight':['balanced', None],\n",
    "    # 'extraTrees__n_estimators':range(80,140, 4),\n",
    "    # 'extraTrees__criterion':[\"gini\", \"entropy\"]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    pipe, parameters,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you are looking at a previous model\n",
    "# model = load('models/jobs/extraTrees_trained_acc76.joblib')[-1]\n",
    "model.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaludate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Metrics\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "plot_roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "plot_confusion_matrix(y_test, model.predict(X_test), labels = ['normal', 'hyper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Model\n",
    "\n",
    "# dump(model, filename='models/jobs/extra_trees_testing.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Disease Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load in disease gene ratio data\n",
    "sle_ratios = pd.read_csv('data/clean/model_validation/sle/sle_features_labels.csv', index_col=0)\n",
    "male_ratios = pd.read_csv('data/clean/model_validation/male/male_features_labels.csv', index_col=0)\n",
    "mace_ratios = pd.read_csv('data/clean/model_validation/mace/mace_features_labels.csv', index_col=0)\n",
    "covid_ratios = pd.read_csv('data/clean/model_validation/covid/covid_features_labels.csv', index_col=0)\n",
    "pad_ratios = pd.read_csv('data/clean/model_validation/pad/pad_features_labels.csv', index_col=0)\n",
    "harp_ratios = pd.read_csv('data/clean/model_validation/harp/harp_features_labels.csv', index_col=0)\n",
    "\n",
    "\n",
    "### Load in disease gene ratio data\n",
    "# sle_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/sle_gene_ratios_filtered_pace_60', index_col=0)\n",
    "# male_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/male_gene_ratios_filtered_pace_60', index_col=0)\n",
    "# mace_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/mace_gene_ratios_filtered_pace_60', index_col=0)\n",
    "# covid_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/covid_gene_ratios_filtered_pace_60', index_col=0)\n",
    "# pad_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/pad_gene_ratios_filtered_pace_60', index_col=0)\n",
    "# harp_ratios = pd.read_csv('data/clean/gene_ratios/disease_ratios/harp_gene_ratios_filtered_pace_60', index_col=0)\n",
    "\n",
    "disease_ratios = {'sle':sle_ratios, 'male':male_ratios, 'mace':mace_ratios, \n",
    "                  'covid':covid_ratios, 'pad':pad_ratios, 'harp':harp_ratios}\n",
    "\n",
    "male_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction Results\n",
    "\n",
    "for key in disease_ratios.keys():\n",
    "    print(f'##################################### Current Disease: {key.upper()} #####################################')\n",
    "    \n",
    "    features = disease_ratios[key].drop('cohort', axis=1)\n",
    "    labels = disease_ratios[key]['cohort']\n",
    "\n",
    "    print(classification_report(labels, model.predict(features)))\n",
    "\n",
    "    plot_roc_curve(labels, model.predict_proba(features)[:,1])\n",
    "\n",
    "\n",
    "        # Predict and graph things\n",
    "    preds = model.predict(features)\n",
    "    conf_matrx = confusion_matrix(labels, preds)\n",
    "    auc_score = roc_auc_score(labels, model.predict_proba(features)[:,1])\n",
    "\n",
    "\n",
    "    sns.heatmap(conf_matrx, xticklabels=[\"Normal\", \"Hyper\"], yticklabels=[\"Healthy\", key],\n",
    "                cmap=\"Blues\", annot=True, fmt='g').set_title(f\"{key} Platelet Activity Correlations \\nAUC: {auc_score:.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Weird results here, so I figure making a histogram of the model is helpful\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"colorblind\")\n",
    "    df = pd.DataFrame({'preds' : model.predict_proba(features)[:,1]})\n",
    "    df['labels'] = labels.values\n",
    "    df['labels'] = df['labels'].map({0:\"Normal\", 1:key})\n",
    "    sns.boxplot(\n",
    "        df, x='labels', y='preds', \n",
    "        order=[\"Normal\", key], boxprops={'alpha': 0.75}\n",
    "        ).set(ylabel=\"Prediction Value\", xlabel=None)\n",
    "    sns.stripplot(\n",
    "        df, x='labels', y='preds',\n",
    "        color = 'black', alpha = 0.75,\n",
    "        order=[\"Normal\", key]\n",
    "        ).set(ylabel=\"Prediction Value\", xlabel=None)\n",
    "\n",
    "    # Significance Testing (kruskal-wallis for nonparametric analysis)    \n",
    "    kruskal_wallis = kruskal(df.loc[df['labels'] == \"Normal\"]['preds'], df.loc[df['labels'] == key]['preds'])\n",
    "    if kruskal_wallis[1] < 0.05:\n",
    "        # statistical annotation\n",
    "        x1, x2 = 0, 1\n",
    "        y, h = df['preds'].max()+0.01, 0.01\n",
    "        plt.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1.2, c='k')\n",
    "        plt.text((x1+x2)*.5, y+h, f\"p = {kruskal_wallis[1]:.4f}\",\n",
    "                 ha='center', va='bottom', color='k')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d66913bb719389121bab79020f7a52c6e05d87274bae621f301af382c19c893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
